{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzc029498-max/nec-/blob/main/Back_Propagation_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    Implementation of a Back-Propagation (BP) Neural Network from scratch.\n",
        "    This version uses 0-based indexing for layers, based on the provided skeleton.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network_architecture, n_epochs, learning_rate, momentum, activation_function, validation_split):\n",
        "        \"\"\"\n",
        "        Initializes the Neural Network.\n",
        "\n",
        "        Args:\n",
        "            network_architecture (list): Units per layer, e.g., [3, 9, 5, 1].\n",
        "                                         Index 0 is input layer, last index is output layer.\n",
        "            n_epochs (int): Number of training epochs.\n",
        "            learning_rate (float): Learning rate (eta).\n",
        "            momentum (float): Momentum (mu).\n",
        "            activation_function (str): 'sigmoid', 'relu', 'linear', 'tanh'.\n",
        "            validation_split (float): Percentage of data for validation (0.0 to 1.0).\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Initialize parameters ---\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = learning_rate\n",
        "        self.mu = momentum\n",
        "        self.fact_name = activation_function\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "        # --- 2. Initialize network structure (using 0-based indexing) ---\n",
        "        self.L = len(network_architecture) # Total number of layers\n",
        "        self.n = network_architecture.copy() # Units per layer\n",
        "\n",
        "        # Array to store activations (xi) and weighted sum inputs (h)\n",
        "        self.xi = [None] * self.L\n",
        "        self.h = [None] * self.L\n",
        "\n",
        "        # Array to store delta errors\n",
        "        self.delta = [None] * self.L\n",
        "\n",
        "        # Weights (w) and Biases (theta) - Indices L-1 for weights connecting L-1 to L\n",
        "        self.w = [None] * self.L\n",
        "        self.theta = [None] * self.L\n",
        "\n",
        "        # Delta weight/bias changes (for current step and previous step)\n",
        "        self.dw = [None] * self.L\n",
        "        self.dtheta = [None] * self.L\n",
        "        self.d_w_prev = [None] * self.L\n",
        "        self.d_theta_prev = [None] * self.L\n",
        "\n",
        "        # --- 3. Initialize weights and arrays ---\n",
        "        for l in range(self.L):\n",
        "            n_units = self.n[l]\n",
        "            self.xi[l] = np.zeros(n_units)\n",
        "            self.h[l] = np.zeros(n_units)\n",
        "            self.delta[l] = np.zeros(n_units)\n",
        "\n",
        "            # Weights and Biases (only for layers l > 0, connecting l-1 to l)\n",
        "            if l > 0:\n",
        "                n_units_prev = self.n[l-1]\n",
        "\n",
        "                # Biases (Theta)\n",
        "                self.theta[l] = np.zeros(n_units)\n",
        "                self.dtheta[l] = np.zeros(n_units)\n",
        "                self.d_theta_prev[l] = np.zeros(n_units)\n",
        "\n",
        "                # Weights Initialization (Glorot/Xavier for Tanh/Sigmoid, He for ReLU)\n",
        "                if self.fact_name == 'relu':\n",
        "                    # He Initialization (Normal distribution)\n",
        "                    limit = np.sqrt(2 / n_units_prev)\n",
        "                    self.w[l] = np.random.normal(0, limit, (n_units, n_units_prev))\n",
        "                else:\n",
        "                    # Glorot/Xavier Initialization (Uniform distribution)\n",
        "                    limit = np.sqrt(6 / (n_units_prev + n_units))\n",
        "                    self.w[l] = np.random.uniform(-limit, limit, (n_units, n_units_prev))\n",
        "\n",
        "                # Initialize delta weight arrays\n",
        "                self.dw[l] = np.zeros((n_units, n_units_prev))\n",
        "                self.d_w_prev[l] = np.zeros((n_units, n_units_prev))\n",
        "\n",
        "        # --- 4. Loss history ---\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the network using input data X and target y.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Split training/validation data ---\n",
        "        if self.validation_split > 0:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=self.validation_split, shuffle=True\n",
        "            )\n",
        "        else:\n",
        "            X_train, y_train = X, y\n",
        "            X_val, y_val = None, None\n",
        "\n",
        "        n_samples = X_train.shape[0]\n",
        "        output_layer_index = self.L - 1\n",
        "\n",
        "        # --- 2. Epoch loop ---\n",
        "        for epoch in range(self.n_epochs):\n",
        "\n",
        "            train_epoch_losses = []\n",
        "\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_train_shuffled = X_train[indices]\n",
        "            y_train_shuffled = y_train[indices]\n",
        "\n",
        "            # --- 3. Sample loop (Stochastic Gradient Descent) ---\n",
        "            for i in range(n_samples):\n",
        "                x_sample = X_train_shuffled[i]\n",
        "                y_sample = y_train_shuffled[i]\n",
        "\n",
        "                self._forward_pass(x_sample)\n",
        "\n",
        "                prediction = self.xi[output_layer_index]\n",
        "                train_epoch_losses.append(self._mse_loss(np.atleast_1d(y_sample), prediction))\n",
        "\n",
        "                self._backward_pass(y_sample)\n",
        "\n",
        "                self._update_weights()\n",
        "\n",
        "            # --- 4. Calculate and store epoch losses ---\n",
        "            avg_train_loss = np.mean(train_epoch_losses)\n",
        "            self.train_loss_history.append(avg_train_loss)\n",
        "\n",
        "            if X_val is not None:\n",
        "                y_val_pred = self.predict(X_val)\n",
        "                avg_val_loss = self._mse_loss(y_val, y_val_pred.ravel())\n",
        "                self.val_loss_history.append(avg_val_loss)\n",
        "            else:\n",
        "                self.val_loss_history.append(np.nan)\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.n_epochs} - \"\n",
        "                      f\"Train Loss: {avg_train_loss:.6f} - \"\n",
        "                      f\"Val Loss: {self.val_loss_history[-1]:.6f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Performs predictions on input data X.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x_sample in X:\n",
        "            self._forward_pass(x_sample)\n",
        "            predictions.append(self.xi[self.L - 1].copy()) # Get activation from the last layer\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def loss_epochs(self):\n",
        "        \"\"\"\n",
        "        Returns the evolution of training and validation error.\n",
        "        \"\"\"\n",
        "        return np.column_stack((self.train_loss_history, self.val_loss_history))\n",
        "\n",
        "    # --------------- Internal Helper Methods ---------------\n",
        "\n",
        "    def _forward_pass(self, x):\n",
        "        \"\"\"Executes the forward pass for a single sample.\"\"\"\n",
        "\n",
        "        output_layer_index = self.L - 1\n",
        "\n",
        "        if len(x) != self.n[0]:\n",
        "            raise ValueError(f\"Input sample size ({len(x)}) does not match input layer size ({self.n[0]})\")\n",
        "\n",
        "        self.xi[0] = x # Set input layer activation\n",
        "\n",
        "        # Iterate from the first hidden layer (index 1) up to the output layer (index L-1)\n",
        "        for l in range(1, self.L):\n",
        "\n",
        "            # Calculate weighted input sum h[l] = w[l] * xi[l-1] + theta[l]\n",
        "            self.h[l] = self.w[l] @ self.xi[l-1] + self.theta[l]\n",
        "\n",
        "            # Use activation function\n",
        "            if l < output_layer_index:\n",
        "                # Hidden layers use the specified activation function\n",
        "                self.xi[l] = self._activation(self.h[l])\n",
        "            else:\n",
        "                # Output layer (l = output_layer_index) for regression uses LINEAR activation\n",
        "                self.xi[l] = self.h[l]\n",
        "\n",
        "    def _backward_pass(self, y_true):\n",
        "        \"\"\"Executes the backward pass for a single sample.\"\"\"\n",
        "\n",
        "        y_true_arr = np.atleast_1d(y_true)\n",
        "        output_layer_index = self.L - 1\n",
        "\n",
        "        # --- 1. Output Layer Delta (L-1) ---\n",
        "        # Error signal\n",
        "        error_signal = y_true_arr - self.xi[output_layer_index]\n",
        "\n",
        "        # Output layer is Linear (regression), so f'(h_L) = 1\n",
        "        self.delta[output_layer_index] = error_signal * np.ones_like(self.h[output_layer_index])\n",
        "\n",
        "\n",
        "        # --- 2. Hidden Layers Delta (L-2 down to 1) ---\n",
        "        # Iterate from the last hidden layer (L-2) down to the first hidden layer (1)\n",
        "        for l in range(output_layer_index - 1, 0, -1):\n",
        "\n",
        "            f_prime_h_l = self._activation_derivative(self.h[l])\n",
        "\n",
        "            # Sum term: delta[l+1] * w[l+1]\n",
        "            sum_term = self.delta[l+1] @ self.w[l+1]\n",
        "\n",
        "            self.delta[l] = sum_term * f_prime_h_l\n",
        "\n",
        "    def _update_weights(self):\n",
        "        \"\"\"Updates weights and thresholds using momentum.\"\"\"\n",
        "\n",
        "        # Iterate from the first connection (index 1) up to the last connection (index L-1)\n",
        "        for l in range(1, self.L):\n",
        "\n",
        "            # Calculate current delta weight and delta bias\n",
        "            self.dw[l] = self.lr * np.outer(self.delta[l], self.xi[l-1])\n",
        "            self.dtheta[l] = self.lr * self.delta[l]\n",
        "\n",
        "            # Update weights and biases with momentum\n",
        "            self.w[l] += self.dw[l] + self.mu * self.d_w_prev[l]\n",
        "            self.theta[l] += self.dtheta[l] + self.mu * self.d_theta_prev[l]\n",
        "\n",
        "            # Store current delta values for next epoch's momentum term\n",
        "            self.d_w_prev[l] = self.dw[l].copy()\n",
        "            self.d_theta_prev[l] = self.dtheta[l].copy()\n",
        "\n",
        "    # --------------- Activation Functions & Derivatives ---------------\n",
        "\n",
        "    def _activation(self, h):\n",
        "        \"\"\"Applies the selected activation function to hidden layers.\"\"\"\n",
        "        if self.fact_name == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-h))\n",
        "        elif self.fact_name == 'relu':\n",
        "            return np.maximum(0, h)\n",
        "        elif self.fact_name == 'tanh':\n",
        "            return np.tanh(h)\n",
        "        else:\n",
        "            # Linear is handled by _forward_pass for the output layer,\n",
        "            # but we include it here for completeness/hidden layers if chosen\n",
        "            return h\n",
        "\n",
        "    def _activation_derivative(self, h):\n",
        "        \"\"\"Calculates the derivative f'(h) of the activation function for hidden layers.\"\"\"\n",
        "        if self.fact_name == 'sigmoid':\n",
        "            f_h = self._activation(h)\n",
        "            return f_h * (1 - f_h)\n",
        "        elif self.fact_name == 'relu':\n",
        "            return (h > 0) * 1.0 # 1 if h > 0, 0 otherwise\n",
        "        elif self.fact_name == 'tanh':\n",
        "            f_h = self._activation(h)\n",
        "            return 1 - f_h**2\n",
        "        else:\n",
        "            return np.ones_like(h) # Derivative of linear is 1\n",
        "\n",
        "    def _mse_loss(self, y_true, y_pred):\n",
        "        \"\"\"Calculates Mean Squared Error (MSE)\"\"\"\n",
        "        return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# --- Example Usage (for testing) ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"--- Testing NeuralNet.py (XOR Example) ---\")\n",
        "\n",
        "    # 1. Prepare simple data (XOR classification/regression)\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([0, 1, 1, 0])\n",
        "\n",
        "    # 2. Define network parameters\n",
        "    architecture = [2, 4, 1] # Input: 2, Hidden: 4, Output: 1\n",
        "    epochs = 5000\n",
        "    lr = 0.1\n",
        "    momentum = 0.9\n",
        "    act_func = 'sigmoid' # Sigmoid works well for XOR\n",
        "    val_split = 0.0\n",
        "\n",
        "    # 3. Create and train the network\n",
        "    nn = NeuralNet(\n",
        "        network_architecture=architecture,\n",
        "        n_epochs=epochs,\n",
        "        learning_rate=lr,\n",
        "        momentum=momentum,\n",
        "        activation_function=act_func,\n",
        "        validation_split=val_split\n",
        "    )\n",
        "\n",
        "    print(f\"\\nArchitecture: {architecture}, Activation: {act_func}, LR: {lr}, Momentum: {momentum}\")\n",
        "    print(\"Starting training...\")\n",
        "    nn.fit(X, y)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # 4. Make predictions\n",
        "    predictions = nn.predict(X)\n",
        "\n",
        "    print(\"\\n--- Predictions ---\")\n",
        "    for x_in, y_t, y_p in zip(X, y, predictions):\n",
        "        print(f\"Input: {x_in} | Target: {y_t} | Predicted: {y_p[0]:.4f}\")\n",
        "\n",
        "    final_loss = nn.loss_epochs()[-1, 0]\n",
        "    print(f\"\\nFinal training loss: {final_loss:.6f}\")\n",
        "\n",
        "    if final_loss < 0.01:\n",
        "        print(\"Test Passed: XOR learned successfully.\")\n",
        "    else:\n",
        "        print(\"Test Failed: Loss is too high.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ROcD_BbK7ViW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
