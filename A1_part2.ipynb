{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1y_FsRmTavIp7XX_WeYjccZcVtlrIQ-WL",
      "authorship_tag": "ABX9TyMzz4X9ckdYllnfosA+WJBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzc029498-max/nec-/blob/main/A1_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeKz5nekQbMr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class NeuralNet:\n",
        "    \"\"\"\n",
        "    Implementation of a Back-Propagation (BP) Neural Network from scratch.\n",
        "\n",
        "    Follows the variable naming and structure defined in the\n",
        "    NEC 2025/26 Activity 1 PDF.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, network_architecture, n_epochs, learning_rate, momentum, activation_function, validation_split):\n",
        "        \"\"\"\n",
        "        Initializes the Neural Network.\n",
        "\n",
        "        Args:\n",
        "            network_architecture (list): Units per layer, e.g., [3, 9, 5, 1]\n",
        "            n_epochs (int): Number of training epochs\n",
        "            learning_rate (float): Learning rate (eta)\n",
        "            momentum (float): Momentum (mu)\n",
        "            activation_function (str): 'sigmoid', 'relu', 'linear', 'tanh'\n",
        "            validation_split (float): Percentage of data for validation (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Initialize parameters ---\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = learning_rate\n",
        "        self.mu = momentum\n",
        "        self.fact_name = activation_function\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "        # --- 2. Initialize network structure (using 1-based indexing) ---\n",
        "\n",
        "        self.L = len(network_architecture)\n",
        "        self.n = [0] + network_architecture\n",
        "\n",
        "        self.h = [None] * (self.L + 1)\n",
        "        self.xi = [None] * (self.L + 1)\n",
        "        self.w = [None] * (self.L + 1)\n",
        "        self.theta = [None] * (self.L + 1)\n",
        "        self.delta = [None] * (self.L + 1)\n",
        "\n",
        "        self.dw = [None] * (self.L + 1)\n",
        "        self.dtheta = [None] * (self.L + 1)\n",
        "\n",
        "        self.d_w_prev = [None] * (self.L + 1)\n",
        "        self.d_theta_prev = [None] * (self.L + 1)\n",
        "\n",
        "        # --- 3. Initialize weights and arrays ---\n",
        "        for l in range(1, self.L + 1):\n",
        "            n_units = self.n[l]\n",
        "\n",
        "            self.h[l] = np.zeros(n_units)\n",
        "            self.xi[l] = np.zeros(n_units)\n",
        "            self.delta[l] = np.zeros(n_units)\n",
        "\n",
        "            if l > 1:\n",
        "                n_units_prev = self.n[l-1]\n",
        "\n",
        "                limit = np.sqrt(6 / (n_units_prev + n_units))\n",
        "                self.w[l] = np.random.uniform(-limit, limit, (n_units, n_units_prev))\n",
        "\n",
        "                self.theta[l] = np.zeros(n_units)\n",
        "\n",
        "                self.dw[l] = np.zeros((n_units, n_units_prev))\n",
        "                self.dtheta[l] = np.zeros(n_units)\n",
        "                self.d_w_prev[l] = np.zeros((n_units, n_units_prev))\n",
        "                self.d_theta_prev[l] = np.zeros(n_units)\n",
        "\n",
        "        # --- 4. Loss history ---\n",
        "        self.train_loss_history = []\n",
        "        self.val_loss_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the network using input data X and target y.\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Split training/validation data ---\n",
        "        if self.validation_split > 0:\n",
        "            X_train, X_val, y_train, y_val = train_test_split(\n",
        "                X, y, test_size=self.validation_split, shuffle=True\n",
        "            )\n",
        "        else:\n",
        "            X_train, y_train = X, y\n",
        "            X_val, y_val = None, None\n",
        "\n",
        "        n_samples = X_train.shape[0]\n",
        "\n",
        "        # --- 2. Epoch loop ---\n",
        "        for epoch in range(self.n_epochs):\n",
        "\n",
        "            train_epoch_losses = []\n",
        "\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_train_shuffled = X_train[indices]\n",
        "            y_train_shuffled = y_train[indices]\n",
        "\n",
        "            # --- 3. Sample loop (Stochastic Gradient Descent) ---\n",
        "            for i in range(n_samples):\n",
        "                x_sample = X_train_shuffled[i]\n",
        "                y_sample = y_train_shuffled[i]\n",
        "\n",
        "                self._forward_pass(x_sample)\n",
        "\n",
        "                prediction = self.xi[self.L]\n",
        "                train_epoch_losses.append(self._mse_loss(np.array([y_sample]), prediction))\n",
        "\n",
        "                self._backward_pass(y_sample)\n",
        "\n",
        "                self._update_weights()\n",
        "\n",
        "            # --- 4. Calculate and store epoch losses ---\n",
        "            avg_train_loss = np.mean(train_epoch_losses)\n",
        "            self.train_loss_history.append(avg_train_loss)\n",
        "\n",
        "            if X_val is not None:\n",
        "                y_val_pred = self.predict(X_val)\n",
        "                avg_val_loss = self._mse_loss(y_val, y_val_pred.ravel())\n",
        "                self.val_loss_history.append(avg_val_loss)\n",
        "            else:\n",
        "                self.val_loss_history.append(np.nan)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.n_epochs} - \"\n",
        "                      f\"Train Loss: {avg_train_loss:.6f} - \"\n",
        "                      f\"Val Loss: {self.val_loss_history[-1]:.6f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Performs predictions on input data X.\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x_sample in X:\n",
        "            self._forward_pass(x_sample)\n",
        "            predictions.append(self.xi[self.L].copy())\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def loss_epochs(self):\n",
        "        \"\"\"\n",
        "        Returns the evolution of training and validation error.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Array of size (n_epochs, 2),\n",
        "                      col 0 is train loss, col 1 is val loss.\n",
        "        \"\"\"\n",
        "        return np.column_stack((self.train_loss_history, self.val_loss_history))\n",
        "\n",
        "    # --------------- Internal Helper Methods ---------------\n",
        "\n",
        "    def _forward_pass(self, x):\n",
        "        \"\"\"Executes the forward pass for a single sample.\"\"\"\n",
        "\n",
        "        if len(x) != self.n[1]:\n",
        "            raise ValueError(f\"Input sample size ({len(x)}) does not match input layer size ({self.n[1]})\")\n",
        "        self.xi[1] = x\n",
        "\n",
        "        for l in range(2, self.L + 1):\n",
        "            self.h[l] = self.w[l] @ self.xi[l-1] + self.theta[l]\n",
        "            self.xi[l] = self._activation(self.h[l])\n",
        "\n",
        "    def _backward_pass(self, y_true):\n",
        "        \"\"\"Executes the backward pass for a single sample.\"\"\"\n",
        "\n",
        "        y_true_val = np.array(y_true).item()\n",
        "\n",
        "        error_signal = y_true_val - self.xi[self.L]\n",
        "\n",
        "        f_prime_h_L = self._activation_derivative(self.h[self.L])\n",
        "\n",
        "        self.delta[self.L] = error_signal * f_prime_h_L\n",
        "\n",
        "        for l in range(self.L - 1, 1, -1):\n",
        "            f_prime_h_l = self._activation_derivative(self.h[l])\n",
        "            sum_term = self.delta[l+1] @ self.w[l+1]\n",
        "            self.delta[l] = sum_term * f_prime_h_l\n",
        "\n",
        "    def _update_weights(self):\n",
        "        \"\"\"Updates weights and thresholds using momentum.\"\"\"\n",
        "\n",
        "        for l in range(2, self.L + 1):\n",
        "\n",
        "            self.dw[l] = self.lr * np.outer(self.delta[l], self.xi[l-1])\n",
        "            self.dtheta[l] = self.lr * self.delta[l]\n",
        "\n",
        "            self.w[l] += self.dw[l] + self.mu * self.d_w_prev[l]\n",
        "\n",
        "            self.theta[l] += self.dtheta[l] + self.mu * self.d_theta_prev[l]\n",
        "\n",
        "            self.d_w_prev[l] = self.dw[l]\n",
        "            self.d_theta_prev[l] = self.dtheta[l]\n",
        "\n",
        "    # --------------- Activation Functions & Derivatives ---------------\n",
        "\n",
        "    def _activation(self, h):\n",
        "        \"\"\"Applies the selected activation function.\"\"\"\n",
        "        if self.fact_name == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-h))\n",
        "        elif self.fact_name == 'relu':\n",
        "            return np.maximum(0, h)\n",
        "        elif self.fact_name == 'linear':\n",
        "            return h\n",
        "        elif self.fact_name == 'tanh':\n",
        "            return np.tanh(h)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function: {self.fact_name}\")\n",
        "\n",
        "    def _activation_derivative(self, h):\n",
        "        \"\"\"Calculates the derivative f'(h) of the activation function.\"\"\"\n",
        "        if self.fact_name == 'sigmoid':\n",
        "            f_h = 1 / (1 + np.exp(-h))\n",
        "            return f_h * (1 - f_h)\n",
        "        elif self.fact_name == 'relu':\n",
        "            return (h > 0) * 1.0\n",
        "        elif self.fact_name == 'linear':\n",
        "            return np.ones_like(h)\n",
        "        elif self.fact_name == 'tanh':\n",
        "            f_h = np.tanh(h)\n",
        "            return 1 - f_h**2\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function: {self.fact_name}\")\n",
        "\n",
        "    def _mse_loss(self, y_true, y_pred):\n",
        "        \"\"\"Calculates Mean Squared Error (MSE)\"\"\"\n",
        "        return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# --- Example Usage (for testing) ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"--- Testing NeuralNet.py ---\")\n",
        "\n",
        "    # 1. Prepare simple data (XOR-like regression)\n",
        "    X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "    y = np.array([0, 1, 1, 0])\n",
        "\n",
        "    print(f\"Test data X shape: {X.shape}\")\n",
        "    print(f\"Test data y shape: {y.shape}\")\n",
        "\n",
        "    # 2. Define network parameters\n",
        "    architecture = [2, 4, 1]\n",
        "    epochs = 5000\n",
        "    lr = 0.1\n",
        "    momentum = 0.9\n",
        "    act_func = 'sigmoid'\n",
        "    val_split = 0.0\n",
        "\n",
        "    # 3. Create and train the network\n",
        "    nn = NeuralNet(\n",
        "        network_architecture=architecture,\n",
        "        n_epochs=epochs,\n",
        "        learning_rate=lr,\n",
        "        momentum=momentum,\n",
        "        activation_function=act_func,\n",
        "        validation_split=val_split\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    nn.fit(X, y)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # 4. Make predictions\n",
        "    predictions = nn.predict(X)\n",
        "\n",
        "    print(\"\\n--- Predictions ---\")\n",
        "    for x_in, y_t, y_p in zip(X, y, predictions):\n",
        "        print(f\"Input: {x_in} | Target: {y_t} | Predicted: {y_p[0]:.4f}\")\n",
        "\n",
        "    # 5. Get loss history\n",
        "    loss_history = nn.loss_epochs()\n",
        "    print(f\"\\nLoss history shape: {loss_history.shape}\")\n",
        "    print(\"Final training loss:\", loss_history[-1, 0])\n",
        "    print(\"Final validation loss:\", loss_history[-1, 1])"
      ]
    }
  ]
}