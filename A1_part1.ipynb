{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_37yIv5o1hz6EYppHNyMY2Askmv7mNkM",
      "authorship_tag": "ABX9TyPYTXr+1pTTB0jZG3SjbIH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzc029498-max/nec-/blob/main/A1_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImEu3u9SOz1o",
        "outputId": "3bde54cc-6c18-4539-eb1d-2a5642845da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the original dataset: 1460 \n",
            "\n",
            "starting processing...\n",
            "\n",
            "Numeric features (7): ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'GrLivArea', 'FullBath', 'BedroomAbvGr']\n",
            "Category-type characteristics (5): ['MSZoning', 'Street', 'LotShape', 'Neighborhood', 'BldgType']\n",
            "\n",
            "Missing values in each column:\n",
            "LotArea         0\n",
            "OverallQual     0\n",
            "OverallCond     0\n",
            "YearBuilt       0\n",
            "GrLivArea       0\n",
            "FullBath        0\n",
            "BedroomAbvGr    0\n",
            "MSZoning        0\n",
            "Street          0\n",
            "LotShape        0\n",
            "Neighborhood    0\n",
            "BldgType        0\n",
            "dtype: int64\n",
            "\n",
            "Data Splitting:\n",
            "Training/Validation Set Size: 1168 (80%)\n",
            "Test set size: 292 (20%)\n",
            "\n",
            "Applying preprocessing to the training/validation set...\n",
            "Applying preprocessing to the test set\n",
            "\n",
            "Preprocessing completed\n",
            "\n",
            "Preprocessing files have been generated.:\n",
            "- preprocessed_data.npz (include X_train_val, y_train_val, X_test, y_test)\n",
            "- feature_names.npy (Includes processed feature names)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "\n",
        "try:\n",
        "    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
        "    df = housing.frame\n",
        "except Exception as e:\n",
        "    print(f\"Unable to load the dataset: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "features_to_use = [\n",
        "\n",
        "    'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'GrLivArea',\n",
        "    'FullBath', 'BedroomAbvGr',\n",
        "\n",
        "    'MSZoning', 'Street', 'LotShape', 'Neighborhood', 'BldgType'\n",
        "]\n",
        "target_variable = 'SalePrice'\n",
        "\n",
        "\n",
        "df = df[features_to_use + [target_variable]].copy()\n",
        "\n",
        "\n",
        "print(f\"Size of the original dataset: {df.shape[0]} \")\n",
        "if df.shape[0] < 1000:\n",
        "    print(\"Warning: The dataset contains fewer than 1,000 samples.\")\n",
        "\n",
        "X = df[features_to_use]\n",
        "y = df[target_variable]\n",
        "\n",
        "\n",
        "print(f\"\\nstarting processing...\\n\")\n",
        "\n",
        "\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "print(f\"Numeric features ({len(numerical_cols)}): {list(numerical_cols)}\")\n",
        "print(f\"Category-type characteristics ({len(categorical_cols)}): {list(categorical_cols)}\")\n",
        "\n",
        "\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(X.isnull().sum())\n",
        "\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nData Splitting:\")\n",
        "print(f\"Training/Validation Set Size: {X_train_val.shape[0]} (80%)\")\n",
        "print(f\"Test set size: {X_test.shape[0]} (20%)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nApplying preprocessing to the training/validation set...\")\n",
        "X_train_val_processed = preprocessor.fit_transform(X_train_val)\n",
        "\n",
        "\n",
        "print(\"Applying preprocessing to the test set\")\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "y_train_val_processed = np.log(y_train_val)\n",
        "y_test_processed = np.log(y_test)\n",
        "\n",
        "print(\"\\nPreprocessing completed\")\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "    np.savez_compressed('preprocessed_data.npz',\n",
        "                        X_train_val=X_train_val_processed,\n",
        "                        y_train_val=y_train_val_processed,\n",
        "                        X_test=X_test_processed,\n",
        "                        y_test=y_test_processed)\n",
        "\n",
        "\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "    np.save('feature_names.npy', feature_names)\n",
        "\n",
        "    print(\"\\nPreprocessing files have been generated.:\")\n",
        "    print(\"- preprocessed_data.npz (include X_train_val, y_train_val, X_test, y_test)\")\n",
        "    print(\"- feature_names.npy (Includes processed feature names)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the file.: {e}\")"
      ]
    }
  ]
}